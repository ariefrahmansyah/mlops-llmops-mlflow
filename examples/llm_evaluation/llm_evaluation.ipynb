{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11ea59fb-e6d6-4919-a835-8c923944acdd",
   "metadata": {},
   "source": [
    "## Define the MLflow Pyfunc model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1535b2c-0dfc-402f-8ad1-b1c184a1a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    "\n",
    "\n",
    "class PyfuncTransformer(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"PyfuncTransformer is a class that extends the mlflow.pyfunc.PythonModel class\n",
    "    and is used to create a custom MLflow model for text generation using Transformers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name, gen_config_dict=None, examples=\"\"):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the PyfuncTransformer class.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the pre-trained Transformer model to use.\n",
    "            gen_config_dict (dict): A dictionary of generation configuration parameters.\n",
    "            examples: examples for multi-shot prompting, prepended to the input.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.gen_config_dict = gen_config_dict if gen_config_dict is not None else {}\n",
    "        self.examples = examples\n",
    "        super().__init__()\n",
    "\n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        Loads the model and tokenizer using the specified model_name.\n",
    "\n",
    "        Args:\n",
    "            context: The MLflow context.\n",
    "        \"\"\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            # device_map=\"auto\"\n",
    "            # make the device CPU\n",
    "            device_map=\"cpu\",\n",
    "        )\n",
    "\n",
    "        # Create a custom GenerationConfig\n",
    "        gcfg = GenerationConfig.from_model_config(model.config)\n",
    "        for key, value in self.gen_config_dict.items():\n",
    "            if hasattr(gcfg, key):\n",
    "                setattr(gcfg, key, value)\n",
    "\n",
    "        # Apply the GenerationConfig to the model's config\n",
    "        model.config.update(gcfg.to_dict())\n",
    "\n",
    "        self.model = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            return_full_text=False,\n",
    "        )\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        \"\"\"\n",
    "        Generates text based on the provided model_input using the loaded model.\n",
    "\n",
    "        Args:\n",
    "            context: The MLflow context.\n",
    "            model_input: The input used for generating the text.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of generated texts.\n",
    "        \"\"\"\n",
    "        if isinstance(model_input, pd.DataFrame):\n",
    "            model_input = model_input.values.flatten().tolist()\n",
    "        elif not isinstance(model_input, list):\n",
    "            model_input = [model_input]\n",
    "\n",
    "        generated_text = []\n",
    "        for input_text in model_input:\n",
    "            output = self.model(self.examples + input_text, return_full_text=False)\n",
    "            generated_text.append(\n",
    "                output[0][\"generated_text\"],\n",
    "            )\n",
    "\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e311ca-ca4b-4d0a-b129-6205e919f500",
   "metadata": {},
   "source": [
    "## Instantiate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a27c87b-3e61-48a5-ae96-4b90fcd3a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcfg = {\n",
    "    \"max_length\": 180,\n",
    "    \"max_new_tokens\": 10,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "example = (\n",
    "    \"Q: Are elephants larger than mice?\\nA: Yes.\\n\\n\"\n",
    "    \"Q: Are mice carnivorous?\\nA: No, mice are typically omnivores.\\n\\n\"\n",
    "    \"Q: What is the average lifespan of an elephant?\\nA: The average lifespan of an elephant in the wild is about 60 to 70 years.\\n\\n\"\n",
    "    \"Q: Which city is known as the 'City of Love'?\\nA: Paris is often referred to as the 'City of Love'.\\n\\n\"\n",
    "    \"Q: What is the capital of Indonesia?\\nA: The capital of Indonesia is Jakarta.\\n\\n\"\n",
    "    \"Q: Who wrote the novel 'The Lord of The Rings'?\\nA: The novel 'The Lord of The Rings' was written by J. R. R. Tolkien.\\n\\n\"\n",
    ")\n",
    "\n",
    "bloomz560 = PyfuncTransformer(\n",
    "    \"bigscience/bloomz-560m\",\n",
    "    gen_config_dict=gcfg,\n",
    "    examples=example,\n",
    ")\n",
    "\n",
    "gpt2large = PyfuncTransformer(\n",
    "    \"gpt2-large\",\n",
    "    gen_config_dict=gcfg,\n",
    "    examples=example,\n",
    ")\n",
    "\n",
    "distilgpt2 = PyfuncTransformer(\n",
    "    \"distilgpt2\",\n",
    "    gen_config_dict=gcfg,\n",
    "    examples=example,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865fc495-06af-4b22-a99c-6e491b4cf796",
   "metadata": {},
   "source": [
    "## Log the models to MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42672fa0-fe0f-47f0-974f-1966fe2af7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(experiment_name=\"compare_llm\")\n",
    "\n",
    "run_ids = []\n",
    "artifact_paths = []\n",
    "model_names = [\"bloomz560\", \"gpt2large\", \"distilgpt2\"]\n",
    "\n",
    "for model, name in zip([bloomz560, gpt2large, distilgpt2], model_names):\n",
    "    with mlflow.start_run(run_name=f\"log_model_{name}\"):\n",
    "        pyfunc_model = model\n",
    "        artifact_path = f\"models/{name}\"\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=artifact_path,\n",
    "            python_model=pyfunc_model,\n",
    "            input_example=\"Q: What color is the sky?\\nA:\",\n",
    "        )\n",
    "        run_ids.append(mlflow.active_run().info.run_id)\n",
    "        artifact_paths.append(artifact_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d682f41-aace-4b96-a0d3-789f8ce8b25c",
   "metadata": {},
   "source": [
    "## Comparing LLMs with MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f91574-3d97-4294-8d92-9ebd70ca8ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(\n",
    "    {\n",
    "        \"question\": [\n",
    "            \"Q: What color is the sky?\\nA:\",\n",
    "            \"Q: Are trees plants or animals?\\nA:\",\n",
    "            \"Q: What is 2+2?\\nA:\",\n",
    "            \"Q: Who is Darth Vader?\\nA:\",\n",
    "            \"Q: What is your favorite color?\\nA:\",\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "print(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62d0512-f20e-4d82-acde-116609e687bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    with mlflow.start_run(run_id=run_ids[i]):  # reopen the run with the stored run ID\n",
    "        evaluation_results = mlflow.evaluate(\n",
    "            model=f\"runs:/{run_ids[i]}/{artifact_paths[i]}\",\n",
    "            model_type=\"text\",\n",
    "            data=eval_df,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83735c70-1c68-4049-8fa8-1430191b38b9",
   "metadata": {},
   "source": [
    "## Recording Generation Parameters with mlflow.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88b673a-87f4-4c30-a335-c53bf866be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "class PyfuncTransformerWithParams(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"PyfuncTransformer is a class that extends the mlflow.pyfunc.PythonModel class\n",
    "    and is used to create a custom MLflow model for text generation using Transformers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the PyfuncTransformer class.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the pre-trained Transformer model to use.\n",
    "            examples: examples for multi-shot prompting, prepended to the input.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        super().__init__()\n",
    "\n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        Loads the model and tokenizer using the specified model_name.\n",
    "\n",
    "        Args:\n",
    "            context: The MLflow context.\n",
    "        \"\"\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(self.model_name, device_map=\"auto\")\n",
    "\n",
    "        self.model = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            return_full_text=False,\n",
    "        )\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        \"\"\"\n",
    "        Generates text based on the provided model_input using the loaded model.\n",
    "\n",
    "        Args:\n",
    "            context: The MLflow context.\n",
    "            model_input: The input used for generating the text.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of generated texts.\n",
    "        \"\"\"\n",
    "        if isinstance(model_input, pd.DataFrame):\n",
    "            model_input = model_input.to_dict(orient=\"records\")\n",
    "        elif not isinstance(model_input, list):\n",
    "            model_input = [model_input]\n",
    "\n",
    "        generated_text = []\n",
    "        for record in model_input:\n",
    "            input_text = record[\"input_text\"]\n",
    "            few_shot_examples = record[\"few_shot_examples\"]\n",
    "            config_dict = record[\"config_dict\"]\n",
    "            # Update the GenerationConfig attributes with the provided config_dict\n",
    "            gcfg = GenerationConfig.from_model_config(self.model.model.config)\n",
    "            for key, value in json.loads(config_dict).items():\n",
    "                if hasattr(gcfg, key):\n",
    "                    setattr(gcfg, key, value)\n",
    "\n",
    "            output = self.model(\n",
    "                few_shot_examples + input_text,\n",
    "                generation_config=gcfg,\n",
    "                return_full_text=False,\n",
    "            )\n",
    "            generated_text.append(output[0][\"generated_text\"])\n",
    "\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f212487-bdec-4c29-933b-4cd75147a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_examples_1 = (\n",
    "    \"Q: Are elephants larger than mice?\\nA: Yes.\\n\\n\"\n",
    "    \"Q: Are mice carnivorous?\\nA: No, mice are typically omnivores.\\n\\n\"\n",
    "    \"Q: What is the average lifespan of an elephant?\\nA: The average lifespan of an elephant in the wild is about 60 to 70 years.\\n\\n\"\n",
    ")\n",
    "\n",
    "few_shot_examples_2 = (\n",
    "    \"Q: Is Mount Everest the highest mountain in the world?\\nA: Yes.\\n\\n\"\n",
    "    \"Q: Which city is known as the 'City of Love'?\\nA: Paris is often referred to as the 'City of Love'.\\n\\n\"\n",
    "    \"Q: What is the capital of Australia?\\nA: The capital of Australia is Canberra.\\n\\n\"\n",
    "    \"Q: Who wrote the novel '1984'?\\nA: The novel '1984' was written by George Orwell.\\n\\n\"\n",
    ")\n",
    "\n",
    "config_dict1 = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_k\": 10,\n",
    "    \"max_length\": 180,\n",
    "    \"max_new_tokens\": 10,\n",
    "}\n",
    "config_dict2 = {\"do_sample\": False, \"max_length\": 180, \"max_new_tokens\": 10}\n",
    "\n",
    "few_shot_examples = [few_shot_examples_1, few_shot_examples_2]\n",
    "config_dicts = [config_dict1, config_dict2]\n",
    "\n",
    "questions = [\n",
    "    \"Q: What color is the sky?\\nA:\",\n",
    "    \"Q: Are trees plants or animals?\\nA:\",\n",
    "    \"Q: What is 2+2?\\nA:\",\n",
    "    \"Q: Who is the Darth Vader?\\nA:\",\n",
    "    \"Q: What is your favorite color?\\nA:\",\n",
    "]\n",
    "\n",
    "data = {\n",
    "    \"input_text\": questions * len(few_shot_examples),\n",
    "    \"few_shot_examples\": [\n",
    "        example for example in few_shot_examples for _ in range(len(questions))\n",
    "    ],\n",
    "    \"config_dict\": [\n",
    "        json.dumps(config) for config in config_dicts for _ in range(len(questions))\n",
    "    ],\n",
    "}\n",
    "\n",
    "eval_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c661fb3e-d4e2-410d-a5e3-ff0839264961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pyfunc model\n",
    "bloomz560_with_params = PyfuncTransformerWithParams(\n",
    "    \"bigscience/bloomz-560m\",\n",
    ")\n",
    "\n",
    "mlflow.set_experiment(experiment_name=\"compare_generation_params\")\n",
    "model_name = \"bloomz560\"\n",
    "\n",
    "with mlflow.start_run(run_name=f\"log_model_{model_name}\"):\n",
    "    # Define an input example\n",
    "    input_example = pd.DataFrame(\n",
    "        {\n",
    "            \"input_text\": \"Q: What color is the sky?\\nA:\",\n",
    "            \"few_shot_examples\": example,  # Assuming 'example' is defined and contains your few-shot prompts\n",
    "            \"config_dict\": {},  # Assuming an empty dict for the generation parameters in this example\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Define the artifact_path\n",
    "    artifact_path = f\"models/{model_name}\"\n",
    "\n",
    "    # log the data\n",
    "    eval_data = mlflow.data.from_pandas(eval_df, name=\"evaluate_configurations\")\n",
    "\n",
    "    # Log the model\n",
    "    mod = mlflow.pyfunc.log_model(\n",
    "        artifact_path=artifact_path,\n",
    "        python_model=bloom560_with_params,\n",
    "        input_example=input_example,\n",
    "    )\n",
    "\n",
    "    # Define the model_uri\n",
    "    model_uri = f\"runs:/{mlflow.active_run().info.run_id}/{artifact_path}\"\n",
    "\n",
    "    # Evaluate the model\n",
    "    mlflow.evaluate(model=model_uri, model_type=\"text\", data=eval_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
