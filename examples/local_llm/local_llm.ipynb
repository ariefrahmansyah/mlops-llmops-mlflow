{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d94e59b3-b611-4af0-b864-8ae85308eefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ariefrahmansyah/src/github.com/ariefrahmansyah/mlops-llmops-mlflow/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cdd334d-d857-4954-8572-aa5cede89dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 19 files: 100%|█████████████████████████████████████████████████| 19/19 [00:00<00:00, 7909.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# Download the  instruct model and tokenizer to a local directory cache\n",
    "snapshot_location = snapshot_download(\n",
    "    repo_id=\"microsoft/Phi-3.5-mini-instruct\", local_dir=\"Phi-3.5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbf1647d-8c31-4fb8-aa43-0d22d35d47ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "\n",
    "class Phi3(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        This method initializes the tokenizer and language model\n",
    "        using the specified model snapshot directory.\n",
    "        \"\"\"\n",
    "        # Initialize tokenizer and language model\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "            context.artifacts[\"snapshot\"], padding_side=\"left\"\n",
    "        )\n",
    "\n",
    "        config = transformers.AutoConfig.from_pretrained(\n",
    "            context.artifacts[\"snapshot\"], trust_remote_code=True\n",
    "        )\n",
    "        # If you are running this in a system that has a sufficiently powerful GPU with available VRAM,\n",
    "        # uncomment the configuration setting below to leverage triton.\n",
    "        # Note that triton dramatically improves the inference speed performance\n",
    "\n",
    "        # config.attn_config[\"attn_impl\"] = \"triton\"\n",
    "\n",
    "        self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "            context.artifacts[\"snapshot\"],\n",
    "            config=config,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        # NB: If you do not have a CUDA-capable device or have torch installed with CUDA support\n",
    "        # this setting will not function correctly. Setting device to 'cpu' is valid, but\n",
    "        # the performance will be very slow.\n",
    "        # self.model.to(device=\"cpu\")\n",
    "        # If running on a GPU-compatible environment, uncomment the following line:\n",
    "        self.model.to(device=\"mps\")\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    def _build_prompt(self, instruction):\n",
    "        \"\"\"\n",
    "        This method generates the prompt for the model.\n",
    "        \"\"\"\n",
    "        INSTRUCTION_KEY = \"### Instruction:\"\n",
    "        RESPONSE_KEY = \"### Response:\"\n",
    "        INTRO_BLURB = (\n",
    "            \"Below is an instruction that describes a task. \"\n",
    "            \"Write a response that appropriately completes the request.\"\n",
    "        )\n",
    "\n",
    "        return f\"\"\"{INTRO_BLURB}\n",
    "        {INSTRUCTION_KEY}\n",
    "        {instruction}\n",
    "        {RESPONSE_KEY}\n",
    "        \"\"\"\n",
    "\n",
    "    def predict(self, context, model_input, params=None):\n",
    "        \"\"\"\n",
    "        This method generates prediction for the given input.\n",
    "        \"\"\"\n",
    "        prompt = model_input[\"prompt\"][0]\n",
    "\n",
    "        # Retrieve or use default values for temperature and max_tokens\n",
    "        temperature = params.get(\"temperature\", 0.1) if params else 0.1\n",
    "        max_tokens = params.get(\"max_tokens\", 1000) if params else 1000\n",
    "\n",
    "        # Build the prompt\n",
    "        prompt = self._build_prompt(prompt)\n",
    "\n",
    "        # Encode the input and generate prediction\n",
    "        # NB: Sending the tokenized inputs to the GPU here explicitly will not work if your system does not have CUDA support.\n",
    "        # If attempting to run this with GPU support, change 'cpu' to 'cuda' for maximum performance\n",
    "        encoded_input = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(\"mps\")\n",
    "        output = self.model.generate(\n",
    "            encoded_input,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            max_new_tokens=max_tokens,\n",
    "        )\n",
    "\n",
    "        # Removing the prompt from the generated text\n",
    "        prompt_length = len(self.tokenizer.encode(prompt, return_tensors=\"pt\")[0])\n",
    "        generated_response = self.tokenizer.decode(\n",
    "            output[0][prompt_length:], skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        return {\"candidates\": [generated_response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b309f5af-010e-4af9-add8-28f1805c73f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types import ColSpec, DataType, ParamSchema, ParamSpec, Schema\n",
    "\n",
    "# Define input and output schema\n",
    "input_schema = Schema(\n",
    "    [\n",
    "        ColSpec(DataType.string, \"prompt\"),\n",
    "    ]\n",
    ")\n",
    "output_schema = Schema([ColSpec(DataType.string, \"candidates\")])\n",
    "\n",
    "parameters = ParamSchema(\n",
    "    [\n",
    "        ParamSpec(\"temperature\", DataType.float, np.float32(0.1), None),\n",
    "        ParamSpec(\"max_tokens\", DataType.integer, np.int32(1000), None),\n",
    "    ]\n",
    ")\n",
    "\n",
    "signature = ModelSignature(\n",
    "    inputs=input_schema, outputs=output_schema, params=parameters\n",
    ")\n",
    "\n",
    "\n",
    "# Define input example\n",
    "input_example = pd.DataFrame({\"prompt\": [\"What is MLflow?\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0c9edc8-e996-4634-ad76-4ce5d8d77893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/207067003305533307', creation_time=1731605153405, experiment_id='207067003305533307', last_update_time=1731605153405, lifecycle_stage='active', name='phi3.5-instruct-evaluation', tags={}>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "\n",
    "mlflow.set_experiment(experiment_name=\"phi3.5-instruct-evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c747bdb-9fa2-49ff-b506-82791d2db80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|███████████████████████████████████████████████| 58/58 [00:04<00:00, 13.90it/s]\n",
      "2024/11/15 00:58:12 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "Downloading artifacts: 100%|███████████████████████████████████████████████| 65/65 [00:59<00:00,  1.09it/s]\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  7.75it/s]\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n",
      "2024/11/15 01:05:12 INFO mlflow.tracking._tracking_service.client: 🏃 View run amazing-ant-818 at: http://127.0.0.1:5000/#/experiments/207067003305533307/runs/743bd5d17b3741abbe5b90006d758d4b.\n",
      "2024/11/15 01:05:12 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/207067003305533307.\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "\n",
    "# Get the current base version of torch that is installed, without specific version modifiers\n",
    "torch_version = torch.__version__.split(\"+\")[0]\n",
    "\n",
    "# Start an MLflow run context and log the PHi3 model wrapper along with the param-included signature to\n",
    "# allow for overriding parameters at inference time\n",
    "with mlflow.start_run():\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        \"phi3.5-instruct\",\n",
    "        python_model=Phi3(),\n",
    "        # NOTE: the artifacts dictionary mapping is critical! This dict is used by the load_context() method in our PHi3() class.\n",
    "        artifacts={\"snapshot\": snapshot_location},\n",
    "        pip_requirements=[\n",
    "            f\"torch=={torch_version}\",\n",
    "            f\"transformers=={transformers.__version__}\",\n",
    "            f\"accelerate=={accelerate.__version__}\",\n",
    "        ],\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc107646-ab36-4694-8312-fc6ce5bcb5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'runs:/743bd5d17b3741abbe5b90006d758d4b/phi3.5-instruct'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info.model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0af902ed-60f6-4ec3-b92f-0cb979d364aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|███████████████████████████████████████████████| 65/65 [01:07<00:00,  1.04s/it]\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.02it/s]\n"
     ]
    }
   ],
   "source": [
    "loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdcd180f-54d7-4f12-97dc-4f412a68d0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'candidates': ['Machine learning is a subset of artificial intelligence that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. It focuses on the development of algorithms that can parse data, learn from it, and then make informed decisions or predictions. These algorithms use statistical techniques to get data to gradually improve their accuracy. Machine learning is widely used in various industries for applications like image and speech recognition, medical diagnosis, stock market trading, and many more.\\n\\n']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(\n",
    "    pd.DataFrame({\"prompt\": [\"What is machine learning?\"]}), params={\"temperature\": 0.6}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
