{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "def63047-cf7a-4f3a-9cc8-5f21125cc5b1",
   "metadata": {},
   "source": [
    "# LLM Evaluation with MLflow\n",
    "\n",
    "* https://mlflow.org/docs/latest/llms/llm-evaluate/notebooks/question-answering-evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7114fa43-8114-4cf6-8f41-1f0abebb592c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "assert \"OPENAI_API_KEY\" in os.environ, \"OPENAI_API_KEY environment variable must be set\"\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36409743-d5ab-4b6f-b7fe-c117e2cfa868",
   "metadata": {},
   "source": [
    "## Basic Question-Answering Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca15307b-66f2-4f8e-b0ce-7142abd50451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import openai\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf4bca61-05b3-425e-af6b-ad55cc32dc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(\n",
    "    {\n",
    "        \"inputs\": [\n",
    "            \"How does useEffect() work?\",\n",
    "            \"What does the static keyword in a function mean?\",\n",
    "            \"What does the 'finally' block in Python do?\",\n",
    "            \"What is the difference between multiprocessing and multithreading?\",\n",
    "        ],\n",
    "        \"ground_truth\": [\n",
    "            \"The useEffect() hook tells React that your component needs to do something after render. React will remember the function you passed (we‚Äôll refer to it as our ‚Äúeffect‚Äù), and call it later after performing the DOM updates.\",\n",
    "            \"Static members belongs to the class, rather than a specific instance. This means that only one instance of a static member exists, even if you create multiple objects of the class, or if you don't create any. It will be shared by all objects.\",\n",
    "            \"'Finally' defines a block of code to run when the try... except...else block is final. The finally block will be executed no matter if the try block raises an error or not.\",\n",
    "            \"Multithreading refers to the ability of a processor to execute multiple threads concurrently, where each thread runs a process. Whereas multiprocessing refers to the ability of a system to run multiple processors in parallel, where each processor can run one or more threads.\",\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbba2940-fbf1-44ac-9550-6915b513f367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/17 12:59:08 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "Downloading artifacts: 100%|‚ñà| 5/5 [00:00<00:00, 3831.81it/s]\n",
      "2024/11/17 12:59:09 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "2024/11/17 12:59:11 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2024/11/17 12:59:11 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2024/11/17 12:59:11 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'toxicity' because it returned None.\n",
      "2024/11/17 12:59:11 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/11/17 12:59:11 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'flesch_kincaid_grade_level' because it returned None.\n",
      "2024/11/17 12:59:11 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/11/17 12:59:11 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'ari_grade_level' because it returned None.\n",
      "2024/11/17 12:59:11 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2024/11/17 12:59:11 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'toxicity' because it returned None.\n",
      "2024/11/17 12:59:11 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/11/17 12:59:11 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'flesch_kincaid_grade_level' because it returned None.\n",
      "2024/11/17 12:59:11 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/11/17 12:59:11 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'ari_grade_level' because it returned None.\n",
      "2024/11/17 12:59:11 INFO mlflow.tracking._tracking_service.client: üèÉ View run honorable-owl-319 at: http://127.0.0.1:5000/#/experiments/771549027936709739/runs/3fa5e21f35bf45b9ad414a41e26bc510.\n",
      "2024/11/17 12:59:11 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://127.0.0.1:5000/#/experiments/771549027936709739.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match/v1': 0.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(experiment_name=\"llm-qa-evaluation\")\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    system_prompt = \"Answer the following question in two sentences\"\n",
    "    basic_qa_model = mlflow.openai.log_model(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        task=openai.chat.completions,\n",
    "        artifact_path=\"model\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"{question}\"},\n",
    "        ],\n",
    "    )\n",
    "    results = mlflow.evaluate(\n",
    "        basic_qa_model.model_uri,\n",
    "        eval_df,\n",
    "        targets=\"ground_truth\",  # specify which column corresponds to the expected output\n",
    "        model_type=\"question-answering\",  # model type indicates which metrics are relevant for this task\n",
    "        evaluators=\"default\",\n",
    "    )\n",
    "results.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8ce2f8e-7262-4692-8991-181088d2b048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà| 1/1 [00:00<00:00, 256.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>outputs</th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does useEffect() work?</td>\n",
       "      <td>The useEffect() hook tells React that your com...</td>\n",
       "      <td>The `useEffect()` hook in React allows you to ...</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What does the static keyword in a function mean?</td>\n",
       "      <td>Static members belongs to the class, rather th...</td>\n",
       "      <td>The `static` keyword in a function means that ...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What does the 'finally' block in Python do?</td>\n",
       "      <td>'Finally' defines a block of code to run when ...</td>\n",
       "      <td>The 'finally' block in Python is used in conju...</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the difference between multiprocessing...</td>\n",
       "      <td>Multithreading refers to the ability of a proc...</td>\n",
       "      <td>Multiprocessing involves running multiple proc...</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              inputs  \\\n",
       "0                         How does useEffect() work?   \n",
       "1   What does the static keyword in a function mean?   \n",
       "2        What does the 'finally' block in Python do?   \n",
       "3  What is the difference between multiprocessing...   \n",
       "\n",
       "                                        ground_truth  \\\n",
       "0  The useEffect() hook tells React that your com...   \n",
       "1  Static members belongs to the class, rather th...   \n",
       "2  'Finally' defines a block of code to run when ...   \n",
       "3  Multithreading refers to the ability of a proc...   \n",
       "\n",
       "                                             outputs  token_count  \n",
       "0  The `useEffect()` hook in React allows you to ...           75  \n",
       "1  The `static` keyword in a function means that ...           49  \n",
       "2  The 'finally' block in Python is used in conju...           62  \n",
       "3  Multiprocessing involves running multiple proc...           65  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.tables[\"eval_results_table\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ed0d2f-5a3e-4898-9304-ff18ea4a47d5",
   "metadata": {},
   "source": [
    "## LLM-judged correctness with OpenAI GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab70fbef-6a8a-461a-9f7d-802e1fd56200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationMetric(name=answer_similarity, greater_is_better=True, long_name=answer_similarity, version=v1, metric_details=\n",
      "Task:\n",
      "You must return the following fields in your response in two lines, one below the other:\n",
      "score: Your numerical score for the model's answer_similarity based on the rubric\n",
      "justification: Your reasoning about the model's answer_similarity score\n",
      "\n",
      "You are an impartial judge. You will be given an input that was sent to a machine\n",
      "learning model, and you will be given an output that the model produced. You\n",
      "may also be given additional information that was used by the model to generate the output.\n",
      "\n",
      "Your task is to determine a numerical score called answer_similarity based on the input and output.\n",
      "A definition of answer_similarity and a grading rubric are provided below.\n",
      "You must use the grading rubric to determine your score. You must also justify your score.\n",
      "\n",
      "Examples could be included below for reference. Make sure to use them as references and to\n",
      "understand them before completing the task.\n",
      "\n",
      "Input:\n",
      "{input}\n",
      "\n",
      "Output:\n",
      "{output}\n",
      "\n",
      "{grading_context_columns}\n",
      "\n",
      "Metric definition:\n",
      "Answer similarity is evaluated on the degree of semantic similarity of the provided output to the provided targets, which is the ground truth. Scores can be assigned based on the gradual similarity in meaning and description to the provided targets, where a higher score indicates greater alignment between the provided output and provided targets.\n",
      "\n",
      "Grading rubric:\n",
      "Answer similarity: Below are the details for different scores:\n",
      "- Score 1: The output has little to no semantic similarity to the provided targets.\n",
      "- Score 2: The output displays partial semantic similarity to the provided targets on some aspects.\n",
      "- Score 3: The output has moderate semantic similarity to the provided targets.\n",
      "- Score 4: The output aligns with the provided targets in most aspects and has substantial semantic similarity.\n",
      "- Score 5: The output closely aligns with the provided targets in all significant aspects.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Example Output:\n",
      "MLflow is an open-source platform for managing machine learning workflows, including experiment tracking, model packaging, versioning, and deployment, simplifying the ML lifecycle.\n",
      "\n",
      "Additional information used by the model:\n",
      "key: targets\n",
      "value:\n",
      "MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.\n",
      "\n",
      "Example score: 4\n",
      "Example justification: The definition effectively explains what MLflow is its purpose, and its developer. It could be more concise for a 5-score.\n",
      "        \n",
      "\n",
      "You must return the following fields in your response in two lines, one below the other:\n",
      "score: Your numerical score for the model's answer_similarity based on the rubric\n",
      "justification: Your reasoning about the model's answer_similarity score\n",
      "\n",
      "Do not add additional new lines. Do not add any other fields.\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "# https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.answer_similarity\n",
    "from mlflow.metrics.genai import EvaluationExample, answer_similarity\n",
    "\n",
    "# Create an example to describe what answer_similarity means like for this problem.\n",
    "example = EvaluationExample(\n",
    "    input=\"What is MLflow?\",\n",
    "    output=\"MLflow is an open-source platform for managing machine \"\n",
    "    \"learning workflows, including experiment tracking, model packaging, \"\n",
    "    \"versioning, and deployment, simplifying the ML lifecycle.\",\n",
    "    score=4,\n",
    "    justification=\"The definition effectively explains what MLflow is \"\n",
    "    \"its purpose, and its developer. It could be more concise for a 5-score.\",\n",
    "    grading_context={\n",
    "        \"targets\": \"MLflow is an open-source platform for managing \"\n",
    "        \"the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, \"\n",
    "        \"a company that specializes in big data and machine learning solutions. MLflow is \"\n",
    "        \"designed to address the challenges that data scientists and machine learning \"\n",
    "        \"engineers face when developing, training, and deploying machine learning models.\"\n",
    "    },\n",
    ")\n",
    "\n",
    "# Construct the metric using OpenAI GPT-4 as the judge\n",
    "answer_similarity_metric = answer_similarity(model=\"openai:/gpt-4\", examples=[example])\n",
    "\n",
    "print(answer_similarity_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "409305b3-a6d0-4d3e-9bd7-bfccea64d261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà| 5/5 [00:00<00:00, 493.33it/s]\n",
      "2024/11/17 12:59:11 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "2024/11/17 12:59:15 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2024/11/17 12:59:15 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2024/11/17 12:59:15 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'toxicity' because it returned None.\n",
      "2024/11/17 12:59:15 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/11/17 12:59:15 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'flesch_kincaid_grade_level' because it returned None.\n",
      "2024/11/17 12:59:15 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/11/17 12:59:15 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'ari_grade_level' because it returned None.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.69s/it]\n",
      "2024/11/17 12:59:17 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2024/11/17 12:59:17 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'toxicity' because it returned None.\n",
      "2024/11/17 12:59:17 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/11/17 12:59:17 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'flesch_kincaid_grade_level' because it returned None.\n",
      "2024/11/17 12:59:17 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/11/17 12:59:17 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'ari_grade_level' because it returned None.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.25it/s]\n",
      "2024/11/17 12:59:21 INFO mlflow.tracking._tracking_service.client: üèÉ View run loud-foal-478 at: http://127.0.0.1:5000/#/experiments/771549027936709739/runs/ba7637fa3e324348ad1e348f5498234f.\n",
      "2024/11/17 12:59:21 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://127.0.0.1:5000/#/experiments/771549027936709739.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match/v1': 0.0,\n",
       " 'answer_similarity/v1/mean': np.float64(3.75),\n",
       " 'answer_similarity/v1/variance': np.float64(1.1875),\n",
       " 'answer_similarity/v1/p90': np.float64(4.7)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    results = mlflow.evaluate(\n",
    "        basic_qa_model.model_uri,\n",
    "        eval_df,\n",
    "        targets=\"ground_truth\",\n",
    "        model_type=\"question-answering\",\n",
    "        evaluators=\"default\",\n",
    "        extra_metrics=[\n",
    "            answer_similarity_metric\n",
    "        ],  # use the answer similarity metric created above\n",
    "    )\n",
    "results.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39272401-455b-4999-aa4d-9d96e4e3bf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà| 1/1 [00:00<00:00, 236.13it/s]\n",
      "Downloading artifacts: 100%|‚ñà‚ñà| 1/1 [00:00<00:00, 359.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>outputs</th>\n",
       "      <th>token_count</th>\n",
       "      <th>answer_similarity/v1/score</th>\n",
       "      <th>answer_similarity/v1/justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does useEffect() work?</td>\n",
       "      <td>The useEffect() hook tells React that your com...</td>\n",
       "      <td>The `useEffect()` hook in React allows you to ...</td>\n",
       "      <td>72</td>\n",
       "      <td>4</td>\n",
       "      <td>The output accurately describes the `useEffect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What does the static keyword in a function mean?</td>\n",
       "      <td>Static members belongs to the class, rather th...</td>\n",
       "      <td>The static keyword in a function indicates tha...</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>The output discusses the 'static' keyword in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What does the 'finally' block in Python do?</td>\n",
       "      <td>'Finally' defines a block of code to run when ...</td>\n",
       "      <td>The 'finally' block in Python is used in excep...</td>\n",
       "      <td>66</td>\n",
       "      <td>5</td>\n",
       "      <td>The model's output closely aligns with the pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the difference between multiprocessing...</td>\n",
       "      <td>Multithreading refers to the ability of a proc...</td>\n",
       "      <td>Multiprocessing involves using multiple proces...</td>\n",
       "      <td>68</td>\n",
       "      <td>4</td>\n",
       "      <td>The output aligns with the provided targets in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              inputs  \\\n",
       "0                         How does useEffect() work?   \n",
       "1   What does the static keyword in a function mean?   \n",
       "2        What does the 'finally' block in Python do?   \n",
       "3  What is the difference between multiprocessing...   \n",
       "\n",
       "                                        ground_truth  \\\n",
       "0  The useEffect() hook tells React that your com...   \n",
       "1  Static members belongs to the class, rather th...   \n",
       "2  'Finally' defines a block of code to run when ...   \n",
       "3  Multithreading refers to the ability of a proc...   \n",
       "\n",
       "                                             outputs  token_count  \\\n",
       "0  The `useEffect()` hook in React allows you to ...           72   \n",
       "1  The static keyword in a function indicates tha...           48   \n",
       "2  The 'finally' block in Python is used in excep...           66   \n",
       "3  Multiprocessing involves using multiple proces...           68   \n",
       "\n",
       "   answer_similarity/v1/score  \\\n",
       "0                           4   \n",
       "1                           2   \n",
       "2                           5   \n",
       "3                           4   \n",
       "\n",
       "                  answer_similarity/v1/justification  \n",
       "0  The output accurately describes the `useEffect...  \n",
       "1  The output discusses the 'static' keyword in t...  \n",
       "2  The model's output closely aligns with the pro...  \n",
       "3  The output aligns with the provided targets in...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.tables[\"eval_results_table\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
